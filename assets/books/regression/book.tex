\documentclass[12pt]{book}

\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters

\usepackage{comment}
\usepackage{mathpazo} % Palatino font
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{array}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{relsize}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage[makeroom]{cancel}

\usepackage[margin=0.5in]{geometry}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\newcommand*\conj[1]{\overbar{#1}}
% STATS Shortcut
\newcommand{\BP}{\Bbb P}
\newcommand{\BR}{\Bbb R}
\let\Bbb\mathbb
\def\sep{\phantom{}}
\newcommand\todo[1]{\phantom{#1}}
\theoremstyle{definition}
\newtheorem{definition}{Définition}[section]
\newtheorem*{example}{Exemple}
\newtheorem{theorem}{Theorême}[section]
\newtheorem{corollary}{Corollaire}[theorem]
\newtheorem{lemma}[theorem]{Lemme}  

\title{Modèles de régression}

\begin{document}
\chapter{Introduction}
Comment modéliser une relation statistique? Existe-il un lien entre deux ou plusieurs variables, et si oui, comment
le représenter. Il est possible lors de la création d'un modèle de régression de modéliser ce type de relation. Un modèle de régression tente de créer un lien mathématique entre ces 
variables et permet même d'estimer à un certain degré de certitude une prévision quant à la valeur que prendra ces 
variables dans certains contextes.

Par exemple, existe-t-il un lien entre le coût d'une main-d'oeuvre et le nombre d'unités produites? Grâce à un modèle de régression
linéaire, on peut décrire et établir ce lien.

\chapter{Régression linéaire simple}
\section{Description du modèle}
\begin{definition}[Régression linéaire]
    \label{def:regression_lineaire}
    Soit $n$ couples de données tel que $(x_i, y_i), i=1 \dots n$. Un modèle de \textbf{régression linéaire} consiste 
    de deux paramètres $\beta_0$ et $\beta_1$ tel que: 
    $$  y_i = \beta_0 + \beta_1 x_i + \epsilon_i, i=1 \dots n $$
\end{definition}

\begin{itemize}
    \item $\beta_0$ est une constante.
    \item $\beta_1$ est un coefficient.
    \item $\epsilon$ est une variable aléatoire qui varie selon l'index $i$ et représente le degré d'erreur du modèle en soit.
\end{itemize}

\begin{example}
    Supposons que l'on cherche d'établir combien nous coûterait la production d'1 million de canards en plastique.
    On a X, le nombre de canards en plastique produit. \sep
    On a Y, le coût en main d'oeuvre. \sep
    On a présentement 5 entrées de données $(n=5)$ basées sur les observations passées dans notre usine. \sep
    Supposons que l'on aie comme données $(x_1 = 500, y_1 = 1000\$)$,$ (x_2 = 10, y_2 = 500\$)$, \dots, $(x_5 = 5000, y_5 = 50400\$)$ \sep 
    On peut déduire un $\beta_0, \beta_1 $ et établir l'intervalle de $\epsilon$. \sep
    Dans cet exemple, $\beta_0 = 400$, $\beta_1 = 10$ \sep 
    et $\epsilon = 0$ ne varie pas. En effet, les valeurs arrivent \textit{exactement} et n'ont donc pas de marge d'erreur.
\end{example}

Supposons

\section{Description des paramètres}
Commençons par définir les deux variables de notre modèle:

\begin{definition}[Variable indépendante]
    \label{def:variable_independante_modele}
    La \textbf{variable indépendante} \hyperref[def:variable_aleatoire]{aléatoire} dans le modèle de régression linéaire pour $n$ couples de données représentée par $X$ et
    peut prendre les valeurs $x_i$
\end{definition}

\begin{definition}[Variable dépendante]
    \label{def:variable_dependante_modele}
    La \textbf{variable dépendante} \hyperref[def:variable_aleatoire]{aléatoire} dans le modèle de régression linéaire pour $n$ couples de données représentée par $Y$ et
    peut prendre les valeurs $y_i$, $i = 1, \dots, n$.
    Si la \hyperref[def:def:regression_lineaire]{régression linéaire} est significative, alors chaque $y_i$ peut être exprimée par 
    $$ y_i = \beta_0 + \beta_1x_i + \epsilon_i $$
\end{definition}

Il est important de faire la différence entre nos données observées (notre échantillon) et l'entiereté de nos données (notre univers).
Le modèle représente la formule pour l'entiereté des données. Donc chaque couple de données de notre univers peut être représentée 
par le modèle en gardant le même $\beta_0$ et le même $\beta_1$ avec $\epsilon_i, x_i$ qui varient. 

Toutefois, afin que le modèle fonctionne, il est crucial de supposer que le terme d'erreur $\epsilon$ suit une loi normale:

\begin{definition}[Terme d'erreur]
    \label{def:terme_erreur}
    Soit $\epsilon_1, \epsilon_2, \dots, \epsilon_n$ représentant le \textbf{terme d'erreur} entre 
    la droite du modèle et la véritable valeur. $\epsilon$ est une \hyperref[def:variable_aleatoire]{variable aléatoire}
    supposée distribuée selon une \hyperref[def:loi_normale]{loi normale} de moyenne nulle et d'une variance $\sigma^2$
    $$ \epsilon \sim N(0, \sigma^2) $$
\end{definition}

On peut ainsi déduire le théorême suivant:
\begin{theorem}
    \label{def:y_loi_normale}
    Pour une valeur $x_i$ fixée de la \hyperref[def:variable_independante_modele]{variable indépendante},
    la \hyperref[def:def:variable_dependante_modele]{variable dépendante} $y_i$ suit une loi normale de moyenne 
    $\beta_0 + \beta_1x_i$ tel que:
    $$ y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2) $$
\end{theorem}

\begin{proof}
    Soit $y_i = \beta_0 + \beta_1x_i + \epsilon_i$. Or, $\epsilon \sim N(0, \sigma^2)$,
    Donc par la \hyperref[def:loi_normale_additivite]{propriété d'additivité de la loi normale}: $y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)$
\end{proof}

N'oublions pas que le modèle ne représente pas une \textit{fonction} à une variable! Il est possible que dans nos couples de données, qu'il existe 
plusieurs valeurs $y$ pour un même $x$. Donc, lorsque nous parlons de la variable aléatoire $y_i$ (et non $Y$), c'est l'ensemble de valeurs que peut 
prendre $y_i$ pour un $x_i$ fixé! E

Puisque la variable aléatoire $y_i$ suit une loi normale et que la loi normale est basée sur la moyenne, on peut déduire que:
\begin{corollary}
    \label{def:y_moyenne}
    La \textbf{moyenne} ou l'\textbf{espérance} de $E(y_i) = \beta_0 + \beta_1x_i$
\end{corollary}


\chapter{Estimation ponctuelle de $\beta_0$ et $\beta_1$}
Supposons que nous voulons créer un \hyperref[def:regression_lineaire]{modèle de régression linéaire} valide pour l'univers de donnée.
Nous désirons trouver une valeur pour $\beta_0$ et $\beta_1$ pour lesquels le modèle fonctionne. Toutefois, nous ne possédons pas 
l'entiereté des données de l'univers, mais seulement un échantillon avec $n$ données. Ainsi, nous n'obtiendrons qu'une \textit{estimation}
de $\beta_0$ et  $\beta_1$, noté $b_0, b_1$ respectivement.

On cherche un $b_0, b_1$ tel que, idéalement, $\epsilon \to 0$. Ainsi:

$$ \hat{y_i} = b_0 + b_1x_i$$ où $\hat{y_i}$ est une valeur théorique et où $\epsilon = 0$ est \textit{aussi} une estimation.

\section{Estimation des moindres carrés}
On cherche à minimiser la différence entre notre $\hat{y_i}$ théorique et notre "vrai" $y_i$.
\begin{lemma}
    \label{lem:estimation_erreur}
    Soit $e$, une \textit{estimation } de l'erreur théorique sur $\hat{y_i}$:
    $y_i = e + \hat{y_i} \implies e = y_i - \hat{y_i}$
\end{lemma}

Soit $y_i$ défini par $(\beta_0, \beta_1)$ et $\hat{y_i}$ défini par $(b_0, b_1)$. La distance entre $y_i$ et $\hat{y_i}$ peut s'exprimer
par la métrique de valeur absolue:
$$ d(y_i, \hat{y_i}) = \sqrt{(y_i - \hat{y_i})^2} = \sqrt{(y_i - b_0 - b_1x_i)} $$

Toutefois il puisque la valeur absolue n'est pas dérivable (et que nous devrons dérivé), nous pouvons
utiliser une métrique carrée: $d(x, y) = (x - y)^2$

Tout d'abord vérifions que c'est une métrique acceptable:
\begin{theorem}
    La fonction en $\Bbb R^2$, $d(x, y) = (x - y)^2$ défini une métrique.
\end{theorem}

\begin{proof}
    $d(x, y) \geq 0 \forall x,y$, car la fonction est toujours positive. \\
    $d(x, y) = 0 \implies x = y$, si $(x - y)^2 = 0 \implies x - y = 0 \implies x = y$ \\
    $x = y \implies d(x, y)$, si $x = y \implies (x - y)^ 2= 0^2 \implies d(x, y) = 0$ \\
    $d(x, y) = d(y, x)$, $(x - y)^2 = x^2 - 2xy + y^2 = (y - x)^2$ \\
    $d(x, y) \leq d(x, z) + d(z, y)$, TODO.
\end{proof}

Ainsi, on cherche le $b_0, b_1$ pour lesquels la distance est minimale pour tout les $x_i, y_i$ et on peut noter cette distance par une fonction 
en $\Bbb R^2$ tel que:
\begin{lemma}
    $$ L(b_0, b_1) = \sum_{i=1}^{n} d(y_i, \hat{y_i}) = \sum_{i=1}^{n} \sqrt{(y_i - b_0 - b_1x_i)} $$
\end{lemma}

Maintenant, il suffit d'\textit{optimiser} la fonction $L(b_0, b_1)$:

\begin{lemma}
    $$ L(b_0, b_1) = \min{L(b_0, b_1)} \iff \frac{\partial L}{\partial b_0} = 0 \land \frac{\partial L}{\partial b_1} = 0 $$
\end{lemma}

En effet, la fonction $L$ est positive pour $\forall b_0,b_1$, et on sait donc qu'il n'existe qu'un point critique et que celui-ci sera le minimum local.
Un diagramme de la fonction aiderait ici.

\begin{lemma}
    $$ \frac{\partial L}{\partial b_0} = 2 $$
\end{lemma}
\begin{proof}
    $\frac{\partial L}{\partial b_0} = \frac{\partial}{\partial b_0} \sum_{i=1}^{n} \sqrt{(y_i - b_0 - b_1x_i)^2} $\\
    $ = \sum_{i=1}^{n} \frac{\partial}{\partial b_0} \sqrt{(y_i - b_0 - b_1x_i)^2} $\\
    $ = \sum_{i=1}^{n} \frac{1}{2 \sqrt{(y_i - b_0 - b_1x_i)^2}} \cdot \frac{\partial}{\partial b_0} (y_i - b_0 - b_1x_i)^2 $\\
    $ = \sum_{i=1}^{n} \frac{1}{2 \sqrt{(y_i - b_0 - b_1x_i)^2}} \cdot 2 (y_i - b_0 - b_1x_i) \cdot \frac{\partial}{\partial b_0} (y_i - b_0 - b_1x_i) $\\
    $ = \sum_{i=1}^{n} \frac{1}{2 \sqrt{(y_i - b_0 - b_1x_i)^2}} \cdot 2 (y_i - b_0 - b_1x_i) \cdot -1 $\\
    $ = \sum_{i=1}^{n} -\frac{(y_i - b_0 - b_1x_i)}{|(y_i - b_0 - b_1x_i)|} $\\
\end{proof}

\section{Estimation quand $\beta_0 = 0$}

\chapter{Intervalle de confiance de $\beta_0$ et $\beta_1$}
\section{Calcul de l'intervalle de $\beta_0$ et $\beta_1$}
\section{Erreur d'estimation}
\begin{definition}[Calcul de l'erreur d'estimation]
    \label{def:calcul_erreur_estimation}
    Soit $alpha$, un XXX \todo{Add name + link} et $E_i$ un erreur d'estimation 
    \todo{Add link} pour $\beta_i$ et $b_i$, le paramètre et son estimateur respectivement.
    $$ P(|\beta_i - b_i| \leq E_i) = 1 - \alpha $$
    En d'autres mots, la différence entre $\beta_i$ et $b_i$ est moindre que l'erreur $E_i$ dans
    $100\% \cdot (1 - \alpha)$ des cas.
\end{definition}
\section{Calcul de la variance}
\subsection{Calcul de la variance de $b_0$}
\subsection{Calcul de la variance de $b_1$}
\chapter{Tests de signification}
Un test de signification est composé de quatre étapes cruciales:
\begin{itemize}
    \item Les hypothèses
    \item La statistique utilisée pour le test
    \item Règle de décision pour déterminer quand on rejette l'\hyperref[def:hypothese_nulle]{hypothèse nulle}
    \item La conclusion du test
\end{itemize}

\section{Test de signification pour $\beta_1$}
Unilatéral vs bilatéral
Rejeter $H_0 \iff $
\section{Test de signification pour $\beta_0$}
On cherche à savoir si le paramètre $\beta_0$ est significatif. Le résultat de ce test 
nous indiquera quelle technique d'estimation utilisée.
\section{Test de signification pour la covariance}
\subsection{Calcul de la covariance de $\beta_0$ avec $\beta_1$}
\chapter{Estimation de la variable dépendante}
Soit le modèle de régression \todo{Add link} $$y_i = \beta_0 + \beta_1x_i + \epsilon_i$$.
On sait que pour un $x_i$ fixé et dans le modèle, $E(\epsilon_i) = 0$. En effet si on avait 
la \textit{vraie} valeur de $\beta_0$ et $\beta_1$, alors $E(\epsilon_i) = 0$ et donc 
$$E(y_i) = \beta_0 + \beta_1x_i$$
\chapter{Prévision de la variable dépendante}
P-Value
\appendix
\chapter{Notions préalables}
Voici une série de définition importantes à considérer lors de la lecture de cet ouvrage. Celles-ci servent
de références pour les exemples et les preuves qui seront amenés dans les chapitres suivants. Pour une explication
plus complète et rigoureuse, il sera nécessaire de se réferrer aux livre précédent: Analyse des Réels.
\begin{definition}[variable aléatoire]
    \label{def:variable_aleatoire}
\end{definition}

\begin{definition}[loi normale]
    \label{def:loi_normale}
    
\end{definition}

\begin{definition}[hypothèse nulle]
    \label{def:hypothese_nulle}
    Soit $H_0$,l'\textbf{hypothèse nulle} représente l'hypothèse représentant le status quo et 
    se note toujours par une égalité: $H_0 = x$.
\end{definition}

\begin{definition}[Erreur de première espèce]
    \label{def:erreur_alpha}
    Soit $\alpha$, représente la probabilité de rejeter mon hypothèse nulle $H_0$ \todo{Add link} sachant
    qu'elle est vraie.
    $$\alpha = P(\text{rejeter }H_0 | H_0\text{ est vraie})$$
\end{definition}

\begin{definition}[Erreur de deuxième espèce]
    \label{def:erreur_beta}
    Soit $\beta$, représente la probabilité d'accepter mon hypothèse nulle $H_0$ \todo{Add link} sachant
    qu'elle est fausse.
    $$\beta = P(\text{accepter }H_0 | H_0\text{ est fausse})$$
\end{definition}

\begin{definition}[Covariance]
    La \textbf{covariance} est l'espérance jointe de $X,Y$ moins l'espérance de $X$ avec l'espérance de $Y$.
    $$ cov(X, Y) = E(XY) - E(X)E(Y) $$
    En effet si les variables $X,Y$ sont indépendantes, alors $E(XY) = E(X)E(Y)$.
\end{definition}

\begin{definition}
    P-value =  probabilité
    $\text{p-value} < \alpha \to \text{ rejette } h_0$
\end{definition}

\end{document}